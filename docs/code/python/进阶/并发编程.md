# 1. 并发编程
## 1.1. 并发编程的概念与目标
并发编程的核心目标是提高程序执行效率，充分利用多核CPU资源，使得多个任务能够在同一时间段内进行处理，达到提高计算机资源利用率的效果。并发编程不仅能改善程序性能，还能在I/O密集型任务中减少等待时间，提升用户体验。

+ **并行（Parallel）**：多个任务在多个CPU核心上同时执行，适用于多核CPU的环境。
+ **并发（Concurrent）**：多个任务在同一个CPU核心上交替执行，看似同时进行，实际上任务是串行执行的，适用于单核CPU。
+ **串行（Serial）**：多个任务依次执行，一个任务完成后，才开始下一个任务。

## 1.2. 实现多任务的三种方式
### 1.2.1. 进程 (Process)
+ **定义**：进程是操作系统中资源分配和独立执行的基本单位。每个进程有独立的内存空间、代码段、数据段、堆栈等，操作系统为每个进程分配资源。
+ **特点**：
+ 进程之间互相独立，彼此不共享内存。
+ 一个进程崩溃不会直接影响到其他进程。
+ 进程的创建和销毁成本较高。
+ **适用场景**：
+ 适合执行完全独立的任务，例如独立的服务进程、计算密集型任务。
+ **注意事项**：
+ 进程之间的通信（IPC，Inter-process Communication）通常比线程复杂，例如通过管道、消息队列或共享内存。

### 1.2.2. 线程 (Thread)
+ **定义**：线程是进程内部的最小执行单元，是操作系统调度和执行的基本单位。一个进程中可以有多个线程，线程共享进程的资源，如内存空间、文件描述符等。
+ **特点**：
+ 线程之间共享进程的内存空间，这使得线程之间的数据交换非常高效。
+ 线程的创建和销毁比进程轻量。
+ 线程间的资源竞争可能导致数据不一致，需使用同步机制（如锁）避免冲突。
+ **适用场景**：
+ 适用于任务紧密相关的并发操作，例如Web服务器、计算密集型任务中的并行处理。
+ **注意事项**：
+ 线程是轻量级的，但仍然需要操作系统进行调度。
+ 线程之间的同步是并发编程的一个重要问题，常用的同步机制有：互斥锁（mutex）、条件变量、读写锁等。

### 1.2.3. 协程 (Coroutine)
+ **定义**：协程是用户态的轻量级线程，由程序员通过代码控制其调度。协程通过让出控制权来模拟多线程的行为，不依赖操作系统的线程调度。
+ **特点**：
+ 协程之间的切换非常轻量，通常不需要操作系统干预。
+ 协程可以在单个线程内实现并发，适合处理大量I/O密集型操作。
+ 协程之间的通信非常高效，因为它们共享内存空间。
+ **适用场景**：
+ 适用于I/O密集型任务，如网络请求、文件操作等。通过非阻塞式I/O操作，协程能够在等待I/O的过程中执行其他任务，从而提升程序的响应速度。
+ **注意事项**：
+ 协程是单线程内的调度，因此无法直接利用多核CPU进行并行计算。对于计算密集型任务，协程的优势不明显。

## 1.3. 各种方式的对比与选择
| 特性 | 进程 | 线程 | 协程 |
| --- | --- | --- | --- |
| 调度单位 | 操作系统 | 操作系统 | 用户代码 |
| 内存空间 | 独立 | 共享 | 共享 |
| 创建/销毁成本 | 高 | 中 | 低 |
| 并发性能 | 较差 | 较好 | 最好 |
| 通信方式 | IPC | 共享内存、同步 | 共享内存 |
| 适用场景 | 完全独立任务 | 密切相关任务 | I/O密集型任务 |


## 1.4. 适用场景与选择策略
+ **进程**适合独立、互不干扰的任务。比如，多个独立服务或计算密集型任务。
+ **线程**适合任务相关、需要共享数据的场景。多线程通常用于需要高并发计算的任务，但要小心同步问题。
+ **协程**适合I/O密集型任务，尤其是需要高并发处理的场景，如高并发的Web服务器、爬虫程序等。

---

## 1.5. 总结
并发编程在现代计算中至关重要，它使得程序能够更高效地利用多核CPU资源。不同的并发模型（进程、线程、协程）各有特点，选择适当的模型可以有效提高程序的性能和响应能力。理解这些模型的工作原理和适用场景，有助于做出最优的技术选择。

---

# 2. 进程（Process）
## 2.1. 进程的定义
+ **狭义**：进程是正在运行的程序实例。
+ **广义**：进程是一个程序在操作系统中的一次执行实例，包含程序、数据、以及系统为执行该程序分配的资源。

## 2.2. 进程与程序的区别
+ **程序**是静态的，它只是存储在磁盘上的代码文件。
+ **进程**是程序的动态实例，是操作系统进行资源分配和管理的基本单位，包含程序代码及其运行时所需的资源。

一个程序可以有多个进程实例，每个进程互相独立执行。

## 2.3. 进程的组成
一个进程主要由以下部分组成：

1. **程序段**：存储在内存中的代码段，是进程执行的指令集。
2. **数据集**：进程执行时所需的数据，包括全局变量和局部变量等。
3. **控制块（PCB）**：进程控制块是操作系统管理进程的关键，它记录进程的状态、程序计数器、寄存器内容等信息。

## 2.4. 进程标识符（PID）
每个进程都有一个唯一的进程ID（PID），操作系统通过PID来区分和管理进程。通过`os.getpid()`和`os.getppid()`可以获取当前进程和父进程的PID。

```python
import os
import time

if __name__ == '__main__':
    for i in range(60):
        time.sleep(1)
        print(i, os.getpid(), os.getppid())
```

---

## 2.5. 进程的调度
进程的调度是操作系统的核心任务，它决定了多个进程如何共享CPU资源。不同的调度算法决定了进程的执行顺序。常见的调度算法有：

1. **FCFS（先来先服务）**：按照进程到达的顺序执行，适合长时间运行的作业。
2. **SJF（短作业优先）**：优先调度执行时间短的作业，可能导致长作业饥饿。
3. **RR（时间片轮转）**：为每个进程分配固定的时间片，适合多任务并发处理。
4. **MFQ（多级反馈队列）**：结合了多种调度算法的优点，适合处理不同类型的任务。

---

## 2.6. 进程状态
进程的状态描述了它在生命周期中的不同阶段。主要有三种状态：

+ **就绪状态（Ready）**：进程已获得执行所需的资源，但正在等待CPU资源。
+ **运行状态（Running）**：进程获得CPU资源并正在执行。
+ **阻塞状态（Blocked）**：进程因等待某些事件（如I/O操作）而无法继续执行，进入阻塞状态。

---

## 2.7. 同步与异步、阻塞与非阻塞
这四个概念有助于理解进程和线程如何处理任务。

#### 2.7.1.1. 5.1 同步与异步
+ **同步（Synchronous）**：任务执行时，后续任务必须等待前一个任务完成。
+ **异步（Asynchronous）**：任务执行时，后续任务可以在等待任务完成的同时执行。

#### 2.7.1.2. 5.2 阻塞与非阻塞
+ **阻塞（Blocking）**：任务等待某个操作完成时，当前任务暂停执行，CPU不执行其他任务。
+ **非阻塞（Non-blocking）**：任务等待某个操作完成时，CPU仍然可以执行其他任务。

#### 2.7.1.3. 5.3 组合
这四种方式可以组合形成不同的执行模式：

+ **同步阻塞**：任务一个接一个地执行，等待时CPU不做其他事。
+ **异步阻塞**：任务可以交替执行，但一旦某个任务阻塞，CPU也会停顿。
+ **同步非阻塞**：任务一个接一个地执行，等待时CPU可以切换到其他任务。
+ **异步非阻塞**：多个任务可以交替执行，任何任务阻塞时，CPU也能继续执行其他任务。

---

## 2.8. Python中的多进程实现
Python提供了多种方式来实现进程管理：

+ `os.fork()`：通过操作系统的`fork`方法创建子进程（仅限Unix系统）。
+ `multiprocessing`：Python内置的多进程模块，提供了创建进程、进程间通信、同步等功能。
+ `subprocess`：用于启动新的进程，并与子进程进行通信。

`multiprocessing`模块提供了创建和管理进程的功能，包括进程池、进程同步等。

---

## 2.9. 总结
本文介绍了并发编程中的进程管理、调度、状态转移、同步与异步的基本概念，尤其关注了Python如何实现多进程。理解这些概念有助于编写更高效、更灵活的并发程序。



# 3. 进程的创建
## 3.1 使用 `os.fork()`
`os.fork()` 是基于 Linux/Unix 系统的系统调用，它用于创建一个子进程。Windows 系统不支持 `fork()`，因此在 Windows 下执行时会报错。

**例子**：

```python
import os
if __name__ == '__main__':
    w = 100
    pid = os.fork()  # 创建子进程
    print(f'当前进程PID: {os.getpid()}')
    if pid == 0:
        print(f'w={w}, 子进程PID={os.getpid()}，当前子进程的父进程PID={os.getppid()}')
    else:
        print(f'当前进程PID：{os.getpid()}，创建了一个子进程，PID={pid}')
```

**解释**：

+ `os.fork()` 创建一个子进程，父进程返回子进程的 PID，子进程返回 0。
+ 子进程会复制父进程的内存空间和资源，但在 `pid == 0` 时可以判断进入子进程执行的部分。

**常用方法**：

| **方法名** | **描述** |
| --- | --- |
| `os.fork()` | 创建子进程，依赖于 Linux 系统的 `fork` 系统调用，在 Windows 下不可用。 |
| `os.getpid()` | 获取当前进程的 PID。 |
| `os.getppid()` | 获取当前进程的父进程 PID。 |


## 3.2使用 `multiprocessing` 模块创建进程
`multiprocessing` 模块是 Python 提供的多进程库，它提供了比 `os.fork()` 更丰富的功能，并且能在 Windows 和 Linux 下跨平台使用。

**例子**：

```python
import os, time
import multiprocessing

def watch():
    for i in range(3):
        print("看电视....", os.getpid())
        time.sleep(1)

if __name__ == '__main__':
    print(os.getpid())  # 输出当前进程的PID
    process = multiprocessing.Process(target=watch)  # 创建一个进程对象，目标函数为 watch
    process.start()  # 启动进程
```

**解释**：

+ `multiprocessing.Process(target=任务)`：`target` 指定要执行的目标函数。
+ 使用 `start()` 启动进程，`join()` 可以等待进程执行完毕。

#### 3.3 Windows 操作系统下创建进程的注意事项
在 Windows 下，Python 使用 `multiprocessing` 模块时会通过导入父进程的代码来创建子进程，因此需要注意以下几点：

1. **Import 语句**：在 Windows 下，如果在创建进程的代码之外执行，会导致父进程的代码被导入并执行，进而导致错误。
2. **解决方法**：需要将创建进程的代码放在 `if __name__ == '__main__':` 判断语句下，以避免递归执行问题。

**修改后的代码**：

```python
import multiprocessing
import os
import time

def watch():
    for i in range(3):
        print("看电视....", os.getpid())
        time.sleep(1)

if __name__ == '__main__':
    process = multiprocessing.Process(target=watch)
    process.start()
```

**解释**：

+ `if __name__ == '__main__':` 确保子进程仅在主程序中启动，而不是在被导入模块时启动。

通过这种结构，Windows 和 Linux/macOS 下都能正常工作。



# 4. multiprocesssing模块
## 4.1. 常用方法
假设p为multiprocessing.Process(target=任务函数/函数方法)的返回值，子进程操作对象。

| **方法名** | **描述** |
| --- | --- |
| **p.start()** | 在主进程中启动子进程p，并调用该子进程p中的run()方法 |
| p.run() | 子进程p启动时运行的方法，去调用start方法的参数target指定的函数/方法。如果要自定义进程类时一定要实现或重写run方法。 |
| p.terminate() | 在主进程中强制终止子进程p，不会进行任何资源回收操作，如果子进程p还创建自己的子进程（孙子进程），则该孙子进程就成了僵尸进程，使用该方法需要特别小心这种情况。如果子进程p还保存了一个锁（lock）那么也将不会被释放，进而导致出现死锁现象。 |
| p.is_alive() | 检测进程是否还存活，如果进程p仍然运行中，返回True |
| **p.join([timeout])** | 主进程交出CPU资源，并阻塞等待子进程结束（强调：是主进程处于等待的状态，而子进程p是处于运行的状态）。timeout是可选的超时时间，需要强调的是，p.join()只能join住start开启的子进程，而不能join住run开启的子进程 |


## 4.2. 常用属性
| **属性名** | **描述** |
| --- | --- |
| **p.daemon** | 默认值为False，如果设为True，代表子进程p作为守护进程在后台运行的，当子进程p的父进程终止时，子进程p也随之终止，并且设定为True后，子进程p不能创建自己的孙子进程的，daemon属性的值必须在p.start()之前设置 |
| p.name | 进程的名称 |
| **p.pid** | 进程的唯一标识符 |


## 4.3. 创建多进程
```python
import multiprocessing
import os
import time

def watch():
    for i in range(3):
        print("看电视....", os.getpid())
        time.sleep(1)

def drink(name, food):
    for i in range(3):
        print(f"{name}喝{food}....",os.getpid())
        time.sleep(1)

def eat(name, food):
    for i in range(3):
        print(f"{name}吃{food}....", os.getpid())
        time.sleep(1)


if __name__ == '__main__':
    print("主进程", os.getpid())
    # 创建三个子进程
    watch_process = multiprocessing.Process(target=watch)
    # 如果希望在父进程创建子进程时，传递数据给子进程的话，可以选择kwargs，args中的任意一种来传递数据，kwargs字典与args元组，可以传递1个到多个数据
    drink_process = multiprocessing.Process(target=drink, kwargs={"name":"小明","food": "羊汤"}) # 命名实参
    eat_process = multiprocessing.Process(target=eat, args=("小明", "米饭", ))  # 位置实参
    # 注意，start表示调用开启进程操作，但是并不会阻塞等待进程真的开启，是一个异步操作。
    watch_process.start()
    drink_process.start()
    eat_process.start()

    print("主程序代码运行结束!")
```

## 4.4. 基于对象写法创建进程
```python
import multiprocessing
import os
import time

class Humen(object):
    def watch(self):
        for i in range(3):
            print("看电视....", os.getpid())
            time.sleep(1)

    def drink(self, food):
        for i in range(3):
            print(f"喝{food}....",os.getpid())
            time.sleep(1)

    def eat(self, food):
        for i in range(3):
            print(f"吃{food}....", os.getpid())
            time.sleep(1)

if __name__ == '__main__':
    xiaoming = Humen()
    print("主进程", os.getpid())
    watch_process = multiprocessing.Process(target=xiaoming.watch)
    drink_process = multiprocessing.Process(target=xiaoming.drink, kwargs={"food": "羊汤"})
    eat_process = multiprocessing.Process(target=xiaoming.eat, args=("米饭", ))

    watch_process.start()
    drink_process.start()
    eat_process.start()
```



## 4.5. 继承process进程类
```python
import os
from multiprocessing import Process


class MyProcess(Process):
    """自定义进程类"""

    def __init__(self, target, name, *args, **kwargs):
        # 父类初始化
        super().__init__(target=target, *args, **kwargs)
        # 自定义参数
        self.name = name

    def run(self):
        """run必须有，但是run中的代码可以根据自己的需要来编写"""
        print(os.getpid())
        print(f'{self.name}子进程运行了')
        super().run()


def func():
    print("子进程的代码")


if __name__ == '__main__':
    p1 = MyProcess(target=func, name='1号')
    p2 = MyProcess(target=func, name='2号')
    p3 = MyProcess(target=func, name='3号')

    p1.start()
    # p1.run() 也可以启动进程，但是工作中，我们一定要使用start()
    p2.start()
    p3.start()

    print("主进程代码运行结束")
```

run也可以保存进程要执行的任务代码，就是把原来的target参数执行要运行的进程任务代码保存到了run方法中，好处就是高内聚，低耦合。代码：

```python
import os
from multiprocessing import Process,current_process


class MyProcess1(Process):
    """自定义进程类"""
    def run(self):
        """run必须有，但是run中的代码可以根据自己的需要来编写"""
        p = current_process()
        print(f"子进程{p.name}的代码1")

class MyProcess2(Process):
    """自定义进程类"""
    def run(self):
        """run必须有，但是run中的代码可以根据自己的需要来编写"""
        p = current_process()
        print(f"子进程{p.name}的代码2")

class MyProcess3(Process):
    """自定义进程类"""
    def run(self):
        """run必须有，但是run中的代码可以根据自己的需要来编写"""
        p = current_process()
        print(f"子进程{p.name}的代码3")

if __name__ == '__main__':
    p1 = MyProcess1(name='1号')
    p2 = MyProcess2(name='2号')
    p3 = MyProcess3(name='3号')

    p1.start()
    # p1.run() 也可以启动进程，但是工作中，我们一定要使用start()
    p2.start()
    p3.start()

    print("主进程代码运行结束")
```

## 4.6. jion方法
在工作中，join方法的作用就是为了监督所有的子进程全部执行结束。

让主进程交出CPU控制权，同步阻塞等待子进程执行结束以后，主进程才进行后续的操作

```python
import time
from multiprocessing import Process


def func(name):
    print('hello', name)
    time.sleep(1)
    print('我是子进程')

if __name__ == '__main__':
    p = Process(target=func, args=('xiaoming',))
    p.start()
    p.join()  # 让主进程交出CPU控制权，同步阻塞等待子进程执行结束以后，主进程才进行后续的操作
    # func()  # 在join阻塞等待过程中，主进程属于同步阻塞状态下，所以这行代码会等待子进程执行结束以后才会报错！
    print('我是主进程，主进程代码执行结束！')
```



## 4.7. 守护进程
守护进程（Daemon Process）也叫精灵进程，是一种特殊的进程，一般在后台运行，不与任何控制终端相关联，并且**周期性地执行**某种任务或**等待处理**某些发生的事件（一般用于处理一些系统级的任务）。

一般在操作系统中，守护进程的进程名称，往往以d结尾的，web服务器的httpd，mysql数据库的mysqld

基本特点：

1. 生存周期长[非必须]，一般启动了以后就会一直驻留在操作系统中，直到主进程结束。
2. 主进程创建了守护进程以后，守护进程作为一个特殊的子进程会随着主进程的代码结束而自动结束。
3. 守护进程内不允许再开子进程（孙子进程）。
4. 守护进程是在后台运行，和终端无关联，不会占着终端，终端可以执行其他命令或操作，终端退出，也不会导致守护进程退出，也因此守护进程中所有关于input等类似的IO操作都不能通过终端来完成。

```python
import time
from multiprocessing import Process


def mydaemon():
    while True:
        print("daemon is alive!")
        time.sleep(1)

if __name__ == '__main__':
    p = Process(target=mydaemon)
    p.daemon = True  # 设置当前子进程为守护进程，必须写在start()方法之前
    p.start()
    time.sleep(5)
```

### 4.7.1. 进程的结束
1. 正常退出（自愿，如用户点击交互式页面的叉号，或程序执行完毕调用发起系统调用正常退出，在linux中用exit，在windows中用ExitProcess）
2. 出错退出（自愿，python中程序要读取一个a.py的内容，但是a.py不存在，此时python解释器会收集错误进行退出）
3. 严重错误（非自愿，执行非法指令，如引用不存在的内存，1/0等，可以捕捉异常，try...except...）
4. 被其他进程杀死（非自愿，如终端下根据进程DI杀死一个进程： kill -9 pid）

多进程的使用过程中， 如果没有正常的结束进程，则会产生僵尸进程或孤儿进程的。

## 4.8. 僵尸进程
如果子进程在exit()之后，父进程没有来得及处理，这时用ps命令就能看到子进程的状态是“Z”。这就是僵尸进程，是一种**有害的进程**，会浪费一定的系统资源，所以在开发中我们一定要**避免出现僵尸进程**。

```python
import time
import multiprocessing


def func():
    print("子进程执行了")
    exit()

if __name__ == '__main__':
    process = multiprocessing.Process(target=func)
    process.start()  # 创建进程
    time.sleep(30)
```



## 4.9. 孤儿进程
孤儿进程会被pid=0的init进程（系统守护进程）所收养，init进程会对所有的孤儿进程进行资源回收，所以孤儿进程不会对系统造成危害。

# 到终端去杀掉主进程，子进程就会变成孤儿进程

```python
import os
import time
from multiprocessing import Process


def func():
    print(f"子进程的pid={os.getpid()}")
    time.sleep(60)
    print("hello")

if __name__ == '__main__':
    print(f"主进程的pid={os.getpid()}")
    p = Process(target=func)
    p.daemon = True
    p.start()
    time.sleep(15)
# 到终端去杀掉主进程，子进程就会变成孤儿进程
```



```shell
ps aux | grep 代码文件名（不要使用中文）

kill -9 主进程PID

# 再次查看就可以发现，子进程变成了一个还在运行的孤儿进程.
ps aux | grep 代码文件名（不要使用中文）
```



## 4.10. p.terminate() 与 p.is_alive()方法
is_alive(): 这个方法返回 True 如果进程当前正在运行，否则返回 False。

terminate(): 这个方法停止进程。它向进程发送一个 SIGTERM 信号，终止进程。

```python
import os
import time
from multiprocessing import Process


def func():
    print(f"子进程")
    time.sleep(3)

if __name__ == '__main__':
    print(f"主进程的pid={os.getpid()}")
    p = Process(target=func)
    print(p.is_alive())  # False
    p.start()  # 异步非阻塞
    print(p.is_alive())  # True
    p.terminate()  # 异步非阻塞
    print(p.is_alive())  # True
    time.sleep(1)
    print(p.is_alive())  # False
```

## 4.11. 当前进程
在 `multiprocessing` 模块中，`current_process()` 函数用于获取当前进程的信息，例如进程名称和进程ID。它可以在进程内存操作中帮助我们识别当前进程对象。

### 示例代码：
```python
import os
import time
from multiprocessing import Process, current_process

def watch():
    p = current_process()
    print(f"当前进程名：{p.name}")
    print(f"当前进程PID：{p.pid}")
    for i in range(3):
        print(f"进程{p.name}在看电视....", os.getpid())

if __name__ == '__main__':
    print("主进程", os.getpid())
    p = Process(target=watch, name="watch")
    p.start()
    time.sleep(30)
```

### 解析：
+ `current_process()` 返回一个当前进程的对象，可以访问 `name` 和 `pid` 属性。
+ `os.getpid()` 获取当前进程的PID。

## 4.12. 锁 (Lock)--防止资源竞争
在并发编程中，多个进程或线程访问共享资源时，可能会导致数据不一致。为了避免这种情况，可以使用 **锁** 来同步多个进程或线程对共享资源的访问。

### 4.12.1. 问题描述
假设我们模拟一个高铁购票程序，多个用户同时查询并尝试购买票。由于并发执行，可能会导致多个进程同时购买同一张票，出现票数超卖的问题。

### 4.12.2. 锁的使用
使用 `multiprocessing` 模块中的 `Lock` 可以对共享资源进行加锁，确保同一时刻只有一个进程修改票数，从而避免数据冲突。

### 4.12.3. 没有锁的情况
```python
import json
import time
from multiprocessing import Process

def get_ticket(username):
    """查询余票"""
    with open("ticket.txt", "r") as f:
        data = json.load(f)
        print(f"{username}查询余票：{data['count']}")

def buy_ticket(username):
    """购买车票"""
    time.sleep(0.1)
    with open("ticket.txt", "r") as f:
        data = json.load(f)
    if data["count"] > 0:
        data["count"] -= 1  # 买票
        print(f"{username}成功购买车票!")
    time.sleep(0.1)
    with open("ticket.txt", "w") as f:
        json.dump(data, f)

def task(username):
    get_ticket(username)
    buy_ticket(username)

if __name__ == '__main__':
    for i in range(10):
        p = Process(target=task, args=(f"user-{i}", ))
        p.start()
```

### 解析：
+ 没有加锁时，多个进程可能会同时读取并修改票数，导致票数超卖。

### 4.12.4. 使用锁解决问题
```python
import json, time
from multiprocessing import Process, Lock

def get_ticket(username):
    """查询余票"""
    time.sleep(0.01)
    with open("ticket.txt", "r") as f:
        data = json.load(f)
    print(f"{username}查询余票：{data['count']}")

def buy_ticket(username):
    """购买车票"""
    time.sleep(0.1)
    with open("ticket.txt", "r") as f:
        data = json.load(f)
    if data["count"] > 0:
        data["count"] -= 1  # 买票
        print(f"{username}购买车票成功!")
    else:
        print(f"{username}购买车票失败!")
    time.sleep(0.1)
    with open("ticket.txt", "w") as f:
        json.dump(data, f)

def task(username, lock):
    """购票流程"""
    get_ticket(username)
    with lock:  # 使用锁确保对票数的修改是互斥的
        buy_ticket(username)

if __name__ == '__main__':
    lock = Lock()
    for i in range(10):
        p = Process(target=task, args=(f"user-{i}", lock))
        p.start()
```

### 解析：
+ 使用 `with lock:` 语句来加锁，确保在修改共享资源 `ticket.txt` 时，只有一个进程能进入 `buy_ticket()`，避免了超卖的问题。

### 4.12.5. 总结
1. **锁的作用**：使用锁保证多个进程访问共享资源时的数据一致性，避免资源冲突。
2. **加锁方式**：使用 `Lock` 来实现进程间的同步。`with lock` 是推荐的加锁方式，它能自动处理锁的获取与释放。
3. **进程间通信**：可以使用 `Queue` 或 `Pipe` 来进行进程间的通信，传递数据。

## 4.13. 进程间数据隔离
在操作系统中，父进程和子进程的数据段是隔离的。子进程在创建时会复制父进程的内存空间，但它们的数据段是独立的。即使父进程和子进程有相同的代码段，它们的内存中的数据是各自独立的。

### 示例代码：
```python
import time, random
from multiprocessing import Process

num = 100

def func():
    global num
    num -= 1

if __name__ == '__main__':
    process_list = []
    for i in range(10):
        p = Process(target=func)
        p.start()
        process_list.append(p)
    t2 = time.time()
    for p in process_list:
        p.join()

    print(num)  # num=?
```

### 解析：
+ 每个子进程都有独立的内存空间，虽然它们共享父进程的代码，但每个子进程对 `num` 的修改不会影响其他进程中的 `num`。

## 4.14. 进程间通信 (IPC)
### 4.14.1. Queue
`Queue` 是一个基于文件类型的通信队列对象，可以用来在进程间传递数据。它支持先进先出（FIFO）机制，并且内置了锁机制来保证数据一致性。

### 示例代码：
```python
from multiprocessing import Process, Queue

def func(exp, queue):
    ret = eval(exp)
    print("eval的计算结果：", ret)
    queue.put(ret)

if __name__ == '__main__':
    q = Queue()
    p = Process(target=func, args=("10+20+30", q)).start()
    print("队列中的结果：", q.get())
```

### 4.14.2. Pipe
`Pipe` 是一个基于文件类型的双向通信管道，允许在两个进程间传递数据。与 `Queue` 不同，`Pipe` 没有先进先出的机制，并且没有内置锁，因此在使用时需要小心同步问题。

### 示例代码：
```python
from multiprocessing import Process, Pipe

def func(exp, con1):
    ret = eval(exp)
    print("eval的计算结果：", ret)
    con1.send(ret)

if __name__ == '__main__':
    con1, con2 = Pipe()
    p = Process(target=func, args=("10+20+30", con1)).start()
    print("管道中的结果：", con2.recv())
```

## 5. pool和子进程
### 5.1. Pool
当需要启动大量的子进程时，直接使用 `Process` 创建进程可能会导致进程创建和管理的开销变得过大。为了提高效率和简化进程管理，可以使用进程池 (`Pool`) 来批量管理子进程。进程池通过创建固定数量的进程来执行任务，避免了频繁创建和销毁进程的性能损耗。

#### 5.1.1. 使用 `Pool` 的优点
+ **进程池大小管理**：通过设置进程池的大小，可以限制同时运行的进程数，避免系统负担过重。
+ **自动管理进程**：进程池会自动分配任务到空闲的进程中，当任务完成后，进程可以继续执行新的任务。
+ **简化代码**：使用 `Pool` 可以减少手动创建和管理多个进程的复杂性。

#### 5.1.2. 示例代码：使用 `Pool` 批量处理任务
```python
from multiprocessing import Pool
import os
import time
import random

def long_time_task(name):
    """模拟一个长时间执行的任务"""
    print('Run task %s (%s)...' % (name, os.getpid()))
    start = time.time()
    time.sleep(random.random() * 3)  # 模拟任务的执行时间
    end = time.time()
    print('Task %s runs %0.2f seconds.' % (name, (end - start)))

if __name__ == '__main__':
    # 创建一个进程池，最多4个进程同时执行
    with Pool(4) as p:
        # 使用 apply_async 异步启动任务
        for i in range(5):
            p.apply_async(long_time_task, args=(i,))
        
        # 关闭进程池，防止新任务的添加
        p.close()
        # 等待所有进程完成
        p.join()
    
    print("所有任务执行完毕")
```

#### 5.1.3. 代码解读：
+ `Pool(4)`：创建一个最多同时执行4个任务的进程池。
+ `apply_async()`：异步执行任务，这样可以同时启动多个任务，进程池会根据空闲进程来调度任务。
+ `close()`：关闭进程池，不再接受新的任务。
+ `join()`：等待所有子进程完成任务。

#### 5.1.4. 进程池的高级用法
+ `apply_async()` 返回一个 `AsyncResult` 对象，可以通过 `get()` 获取任务执行的结果。
+ `map()`：类似于内置的 `map()` 函数，但可以并行执行任务。

示例代码：

```python
from multiprocessing import Pool

def square(x):
    return x * x

if __name__ == '__main__':
    with Pool(4) as p:
        results = p.map(square, range(10))
    print(results)
```

### 5.2. 子进程管理 (`subprocess` 模块)
在 Python 中，`subprocess` 模块可以用来启动和管理外部进程。通过 `subprocess`，你可以控制子进程的输入、输出和错误流，也可以获取子进程的执行结果。

#### 5.2.1. 常用 `subprocess` 方法
+ `subprocess.call()`：启动子进程并等待其执行完毕，返回子进程的退出码。
+ `subprocess.run()`：Python 3.5+ 中推荐的方法，功能与 `call()` 相似，但返回一个 `CompletedProcess` 对象，包含更多的信息，如标准输出和标准错误。
+ `subprocess.Popen()`：更底层的接口，可以让你更灵活地控制子进程的输入、输出和错误流。

#### 5.2.2. 示例代码：使用 `subprocess.call()` 启动外部命令
```python
import subprocess

# 调用 nslookup 查询域名解析结果
print('$ nslookup www.python.org')
r = subprocess.call(['nslookup', 'www.python.org'])
print('Exit code:', r)
```

#### 5.2.3. 示例代码：使用 `subprocess.run()` 获取输出
```python
import subprocess

# 调用 ls 命令列出当前目录的文件
result = subprocess.run(['ls', '-l'], capture_output=True, text=True)
print('命令输出:\n', result.stdout)
print('错误输出:\n', result.stderr)
```

+ `capture_output=True`：捕获子进程的标准输出和错误输出。
+ `text=True`：将输出转换为字符串，而不是字节数据。

#### 5.2.4. 示例代码：使用 `subprocess.Popen()` 控制子进程的输入/输出
```python
import subprocess

# 使用 Popen 启动一个进程并与其交互
process = subprocess.Popen(
    ['grep', 'hello'], 
    stdin=subprocess.PIPE, 
    stdout=subprocess.PIPE, 
    text=True
)

# 向子进程发送数据
stdout, stderr = process.communicate(input="hello world\nhello python\nhello subprocess\n")

# 获取并输出子进程的结果
print('子进程输出:\n', stdout)
```

在这个例子中，`Popen` 启动了一个 `grep` 命令，向它的标准输入流发送数据，然后从标准输出流读取结果。

#### 5.2.5. 错误处理和调试
`subprocess` 模块也提供了错误处理功能，例如通过 `check_call()` 或 `check_output()` 来捕获错误。`subprocess.run()` 也提供了 `check=True` 参数来在命令失败时抛出 `subprocess.CalledProcessError` 异常。

```python
import subprocess

try:
    subprocess.run(['nonexistent_command'], check=True)
except subprocess.CalledProcessError as e:
    print(f"命令执行失败，返回码：{e.returncode}")
```

### 5.3. 小结
+ `Pool` 是一个高效的进程池管理工具，适用于需要启动大量子进程的情况，可以帮助减少进程创建和销毁的开销。
+ `subprocess` 是启动和管理外部进程的强大工具，提供了灵活的输入/输出控制，可以用于自动化脚本或与外部程序交互。
+ 使用进程池可以更好地控制并发任务的数量，而 `subprocess` 则更适合与外部命令或程序的交互。

在实际应用中，选择合适的进程管理方式可以有效提高程序的性能和可维护性。6. 进程总结

## 进程也存在着很多不足之处：
+ 单个进程内只能在同一个时间干一件事，如果想同时干两件事或多件事，进程就无能为力了。
+ 进程在执行的过程中如果阻塞，例如等待输入，整个进程就会挂起，即使进程中有些工作不依赖于输入的数据，也将无法执行。
+ 操作系统在每个进程的创建、管理、切换、回收等操作上，需要耗费一部分的计算机资源，而如果在多任务场景下，多个进程替换调度比较频繁的话，那么CPU就会浪费大量的资源在切换调度进程与进程的创建和回收等操作上面了。

基于进程的这些不足之处，所以计算机中又出现了线程的概念。

## 5. 多线程
### 5.1. 多线程概述
+ **多任务**：可以通过多个进程或同一进程中的多个线程实现。
+ **线程与进程**：一个进程至少有一个线程，多个线程共享进程内的资源（如内存），因此多线程比多进程更轻量。
+ **Python中的线程**：Python提供了`_thread`和`threading`模块，`threading`是高级模块，适用于大多数情况。

---

### 5.2. 创建线程
```python
import time
import threading

def loop():
    """这是一个简单的循环任务，模拟线程执行的操作。"""
    print(f'thread {threading.current_thread().name} is running...')
    for n in range(1, 6):
        print(f'thread {threading.current_thread().name} >>> {n}')
        time.sleep(1)  # 模拟耗时任务
    print(f'thread {threading.current_thread().name} ended.')

# 主线程执行的一部分
print(f'thread {threading.current_thread().name} is running...')
t = threading.Thread(target=loop, name='LoopThread')  # 创建新线程，目标函数为loop
t.start()  # 启动线程
t.join()  # 等待线程结束
print(f'thread {threading.current_thread().name} ended.')
```

#### 讲解：
+ **线程创建**：通过 `threading.Thread` 创建线程时，我们指定了一个目标函数 `loop`，该函数将在新线程中执行。
+ **主线程执行**：主线程会打印出自己开始执行的消息，然后启动新线程 `t.start()`。
+ **线程同步**：`t.join()` 会阻塞主线程，直到新线程 `t` 完成执行后才继续执行主线程的后续代码。这样主线程会等待 `LoopThread` 结束后再结束。

#### 输出结果：
```plain
thread MainThread is running...
thread LoopThread is running...
thread LoopThread >>> 1
thread LoopThread >>> 2
thread LoopThread >>> 3
thread LoopThread >>> 4
thread LoopThread >>> 5
thread LoopThread ended.
thread MainThread ended.
```

---

### 5.3. 锁（Lock）
#### 5.3.1with lock示例
```python
import time
import threading

balance = 0  # 假设这是共享资源
lock = threading.Lock()  # 创建锁

def change_it(n):
    """模拟对共享资源的修改"""
    global balance
    balance = balance + n
    balance = balance - n

def run_thread(n):
    """每个线程执行100000次修改操作"""
    for i in range(100000):
        with lock:  # 使用with语句自动获取和释放锁
            change_it(n)

# 创建两个线程
t1 = threading.Thread(target=run_thread, args=(5,))
t2 = threading.Thread(target=run_thread, args=(8,))
t1.start()  # 启动线程1
t2.start()  # 启动线程2

t1.join()  # 等待线程1完成
t2.join()  # 等待线程2完成

print(f'Final balance: {balance}')
```

#### 讲解：
+ `with lock`：使用 `with lock` 语法可以确保锁的获取和释放过程更加简洁，避免遗漏释放锁。`with` 语句会在代码块执行完毕后自动释放锁。
+ **共享资源**：两个线程对同一个全局变量 `balance` 进行修改。由于没有锁，可能会发生竞争条件，导致 `balance` 的最终值不符合预期。通过使用 `Lock`，我们确保了每次只有一个线程可以访问 `balance`，从而避免了竞态问题。

#### 输出（可能会有小的波动）：
```plain
Final balance: 0

```

---

#### 5.3.2release，acquire
```python
import time, threading

balance = 0  # 共享变量
lock = threading.Lock()  # 创建锁

def change_it(n):
    global balance
    balance = balance + n
    balance = balance - n

def run_thread(n):
    for i in range(100000):
        lock.acquire()  # 获取锁
        try:
            change_it(n)
        finally:
            lock.release()  # 释放锁

t1 = threading.Thread(target=run_thread, args=(5,))
t2 = threading.Thread(target=run_thread, args=(8,))
t1.start()
t2.start()
t1.join()
t2.join()
print(balance)  # 输出最终的 balance

```

#### 解释：
+ `lock.acquire()` 获取锁，防止其他线程同时访问共享变量。
+ `lock.release()` 释放锁，允许其他线程访问。
+ 使用锁保证了对共享资源 `balance` 的访问是安全的。

### 5.4. GIL（全局解释器锁）
#### GIL解释：
```python
import threading
import time

def cpu_bound_task():
    """一个消耗CPU的任务"""
    count = 0
    while count < 1000000:
        count += 1
    print(f'Finished task in {threading.current_thread().name}')

# 启动多个线程执行CPU密集型任务
threads = []
for i in range(4):
    t = threading.Thread(target=cpu_bound_task, name=f'Thread-{i+1}')
    t.start()
    threads.append(t)

for t in threads:
    t.join()  # 等待所有线程完成
```

#### 讲解：
+ **CPU密集型任务**：该代码启动了多个线程来执行一个计算密集型任务，即增加 `count` 的值。理论上，这应该占用大量CPU资源。
+ **GIL的作用**：尽管我们创建了多个线程，但由于GIL的存在，这些线程不会在多个CPU核心上并行执行。Python的GIL确保在同一时刻只有一个线程能够执行Python字节码，这意味着即使有多个线程，CPU密集型任务依然只能在一个核心上执行。

#### 输出（每个线程会依次完成任务）：
```plain
Finished task in Thread-1
Finished task in Thread-2
Finished task in Thread-3
Finished task in Thread-4
```

---

通过这种优化和详细解释，代码和原理可以更清晰地呈现。希望这对您有帮助！如果还有其他方面需要进一步修改或优化，请随时告诉我。

### 5.5. 小结
+ **多线程**适用于I/O密集型任务，但由于GIL的限制，**Python的多线程不能有效利用多核CPU**。
+ **锁**用于保证线程安全，但也会影响程序的执行效率。
+ 在多核环境下，如果需要充分利用CPU的多个核心，应该使用**多进程**而非多线程。

---

## 6. ThreadLocal 概述
在多线程编程中，每个线程通常会有自己的独立数据，这些数据对其他线程不可见。使用局部变量相较于全局变量的一个重要优势是**线程隔离性**，避免了对全局变量的并发修改。

然而，在多层函数调用中传递局部变量可能变得繁琐。如果每个函数都需要传递某个对象作为参数，就会显得冗长且不优雅，尤其是在多线程中，这种数据传递更为复杂。

### 6.1. 问题场景
假设每个线程处理不同的对象（例如`Student`对象），我们需要在多个函数之间共享数据。常见做法是将数据存储在全局字典中，通过当前线程作为key来查找对应的对象：

```python
global_dict = {}

def std_thread(name):
    std = Student(name)
    global_dict[threading.current_thread()] = std
    do_task_1()
    do_task_2()

def do_task_1():
    std = global_dict[threading.current_thread()]
    ...

def do_task_2():
    std = global_dict[threading.current_thread()]
    ...
```

尽管这种方法可行，但每个函数都需要显式地从全局字典中查找数据，代码变得不够简洁。

### 6.2. 使用 `ThreadLocal`
`ThreadLocal` 是 Python 提供的一个便捷类，它可以帮助每个线程绑定自己的数据，简化多线程环境下的局部变量管理。使用 `ThreadLocal`，每个线程都可以通过`local_school`对象绑定自己的数据，而无需显式地传递或查找数据。

#### 6.2.1. 示例：
```python
import threading

# 创建全局ThreadLocal对象:
local_school = threading.local()

def process_student():
    # 获取当前线程关联的student:
    std = local_school.student
    print(f'Hello, {std} (in {threading.current_thread().name})')

def process_thread(name):
    # 绑定ThreadLocal的student:
    local_school.student = name
    process_student()

t1 = threading.Thread(target=process_thread, args=('Alice',), name='Thread-A')
t2 = threading.Thread(target=process_thread, args=('Bob',), name='Thread-B')
t1.start()
t2.start()
t1.join()
t2.join()
```

#### 6.2.2. 输出结果：
```plain
Hello, Alice (in Thread-A)
Hello, Bob (in Thread-B)
```

解释

+ `local_school` 是一个 `ThreadLocal` 对象，每个线程都能绑定自己的 `student` 属性，彼此之间的数据互不干扰。
+ `ThreadLocal` 可以被看作是一个线程局部存储的**字典**，每个线程可以读取和修改自己的变量（例如 `local_school.student`）。
+ 通过 `ThreadLocal`，不再需要显式地将对象传递给每个函数，简化了多线程编程的复杂度。

### 6.3. 常见应用
`ThreadLocal` 最常用于以下场景：

+ 为每个线程绑定一个独立的数据库连接。
+ 存储与每个HTTP请求相关的用户身份信息。
+ 管理每个线程独立的会话数据等。

### 6.4. 小结
+ **ThreadLocal** 解决了线程间共享数据的并发问题，提供了线程独立的数据存储，每个线程拥有自己独立的副本。
+ 它简化了参数传递问题，尤其是在多层函数调用的场景中。
+ 可以用于管理线程相关的资源，如数据库连接、用户信息等。

---

